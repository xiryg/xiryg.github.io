[{"id":"58db483b0e32a8b92b0e85dca7e985c8","title":"西瓜书+南瓜书 第6章 支持向量机","content":"1. 支持向量机概述\r\n支持向量机是一种监督学习算法，主要用于分类和回归分析。它在解决小样本、非线性以及高维数问题上表现出色。\r\n2. 基本概念\r\n2.1 间隔（Margin）\r\n间隔是数据点到决策边界（超平面）的距离。间隔越大，模型的泛化能力通常越强。\r\n2.2 函数间隔与几何间隔\r\n\r\n函数间隔：超平面关于样本点的函数间隔为 \r\n。\r\n几何间隔：超平面的几何间隔定义为  。\r\n\r\n3. 最大间隔与支持向量\r\n\r\n最大间隔分类器：目标是找到能够最大化几何间隔的超平面。\r\n支持向量：满足  的样本点，它们是距离超平面最近的点。\r\n\r\n4. 最优化问题\r\n4.1 原始问题\r\n最大化几何间隔可以转化为最小化  ，约束条件为  ## 4.2\r\n对偶问题 通过拉格朗日乘子法，将原始问题转化为对偶问题：  其中，拉格朗日函数  。\r\n5. 核函数\r\n5.1 引入核函数\r\n核函数允许我们在高维空间中处理线性不可分的数据，而无需显式地映射到高维空间。\r\n5.2 常用核函数\r\n\r\n线性核： \r\n多项式核： \r\n高斯核（径向基函数）： \r\nSigmoid核： \r\n\r\n6. 软间隔与支持向量回归\r\n6.1 软间隔\r\n为了处理数据中的噪声，引入软间隔的概念，允许一些数据点违反间隔的约束。\r\n6.2 优化问题\r\n软间隔SVM的优化问题引入松弛变量，目标函数变为：  约束条件为  。\r\n7. SMO算法\r\n序列最小优化算法（SMO）是一种用于求解SVM对偶问题的有效算法，它通过每次只优化两个拉格朗日乘子来简化问题。\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第6章-支持向量机","date":"2024-08-19T12:11:57.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习,支持向量机","author_index":"xiryg"},{"id":"264ded5e258fbeb5d658affa9fae34d0","title":"西瓜书+南瓜书 第5章 神经网络","content":"1. M-P神经元模型\r\nM-P神经元（McCulloch-Pitts\r\nNeuron）是神经网络的基础单元。它接收多个输入信号，通过权重加权求和，并与阈值进行比较，然后通过激活函数得到输出。\r\n公式表示如下： \r\n其中： -  ：输入信号 - ：输入信号的权重 - ：神经元的阈值 - ：激活函数\r\n常见的激活函数包括： - 阶跃函数：\r\n\r\n\r\nSigmoid函数：\r\n\r\n\r\n\r\nTanh函数：\r\n\r\n\r\n\r\nReLU函数：\r\n\r\n\r\n2. 感知机模型\r\n感知机（Perceptron）是一种简单的线性分类器，其激活函数为阶跃函数。感知机通过学习一个线性决策边界来对输入数据进行分类。\r\n感知机的公式如下：\r\n\r\n感知机的目标是找到一个超平面，将数据集中的样本点正确分类。其学习算法基于梯度下降法，对误分类样本进行权重更新：\r\n\r\n其中： - ：学习率 - ：真实标签 - ：预测标签\r\n感知机模型只能处理线性可分的数据，对于非线性可分的数据，需要多层神经网络来解决。\r\n3. 多层前馈神经网络（MLP）\r\n多层前馈神经网络（Multi-Layer Perceptron,\r\nMLP）由输入层、一个或多个隐藏层和输出层组成。每层的神经元与下一层的神经元全连接。\r\nMLP中的每个神经元都包含一个激活函数，常见的激活函数包括Sigmoid函数和ReLU函数。通过引入隐藏层，MLP能够处理非线性可分的数据。\r\n4. 反向传播算法（BP算法）\r\n反向传播算法（Backpropagation）用于训练多层前馈神经网络。其基本思想是通过梯度下降法，最小化预测输出与真实标签之间的误差。BP算法分为两个阶段：前向传播和反向传播。\r\n\r\n前向传播：计算每个神经元的输出。\r\n计算误差：使用损失函数（如均方误差）计算输出误差。\r\n反向传播：将误差从输出层反向传播到输入层，计算每个权重的梯度。\r\n更新权重：使用梯度下降法调整权重。\r\n\r\n损失函数为均方误差（MSE）：\r\n\r\n权重的更新公式为：\r\n\r\n其中： - ：学习率 - ：损失函数对权重的偏导数\r\n5. 深度学习\r\n深度学习（Deep\r\nLearning）是指具有多个隐藏层的神经网络。通过增加网络的深度，可以提高模型的表达能力，解决更复杂的任务。\r\n常见的深度学习模型包括： -\r\n卷积神经网络（Convolutional Neural Network,\r\nCNN）：主要用于图像处理。 - 递归神经网络（Recurrent\r\nNeural Network, RNN）：主要用于序列数据处理。\r\n深度学习中的两个重要策略： 1.\r\n无监督逐层训练：逐层预训练，然后进行全局微调。 2.\r\n权重共享：同一层神经元使用相同的连接权重，典型的例子是CNN。\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n#\r\nActivation Functions\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第5章-神经网络","date":"2024-08-19T12:11:46.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习,神经网络","author_index":"xiryg"},{"id":"7822ee47590d9e0b45749148ae7e7cac","title":"西瓜书+南瓜书 第4章 决策树","content":"1. 决策树基本概念\r\n决策树是一种基于树状结构进行决策的分类和回归模型。通过对数据进行属性测试，将样本分成不同的类别，最终形成叶子节点。每个非叶节点表示一个属性测试，每个叶子节点表示一个分类结果。\r\n2. 决策树的构造\r\n决策树的构造是一个递归过程，分为以下几个步骤：\r\n\r\n根节点：选择一个最优属性作为根节点。\r\n属性选择：对于每个内部节点，选择一个最优属性进行测试。\r\n递归划分：对选定属性的每个可能结果，递归地构建子树。\r\n停止条件：当满足以下任一条件时停止：\r\n\r\n当前节点包含的样本全属于同一类别。\r\n当前属性集为空或所有样本在所有属性上取值相同。\r\n当前节点包含的样本集合为空。\r\n\r\n\r\n\r\n选择划分属性的目标是使得子节点尽可能纯，即同类样本尽可能多。\r\n常用的决策树算法有ID3、C4.5和CART。\r\n2.1 ID3算法\r\nID3算法通过信息增益选择划分属性。信息增益基于信息熵计算：\r\n\r\n其中， 表示第  类样本在数据集  中的概率。\r\n信息增益定义为划分前后的信息熵差值：\r\n\r\n其中，  是在属性  上取值为  的子集，  是属性  的所有可能取值。\r\n2.2 C4.5算法\r\nC4.5算法通过增益率选择划分属性，以避免偏向于取值较多的属性。增益率定义为：\r\n\r\n其中， \r\n定义为：\r\n\r\n2.3 CART算法\r\nCART决策树使用基尼指数来选择划分属性，基尼指数反映了随机抽取两个样本类别不一致的概率：\r\n\r\n属性  的基尼指数为：\r\n\r\n3. 剪枝处理\r\n决策树容易出现过拟合现象，通过剪枝可以缓解这种情况。剪枝策略包括预剪枝和后剪枝。\r\n3.1 预剪枝\r\n在构造过程中评估节点是否需要分支，如果分支对模型性能没有提升，则不进行分支。\r\n3.2 后剪枝\r\n构造完整决策树后，从底向上评估分支的必要性，如果分支对模型性能没有提升，则剪掉该分支。\r\n4. 连续值与缺失值处理\r\n4.1 连续值处理\r\n对连续值属性进行离散化处理，常用的方法是二分法，选择最优划分点进行划分。\r\n4.2 缺失值处理\r\n对缺失值的处理方法包括：\r\n\r\n通过样本子集计算信息增益。\r\n将样本按权重划分到各个分支中。\r\n\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第4章-决策树","date":"2024-08-19T12:11:25.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习,决策树","author_index":"xiryg"},{"id":"73a97f2b305d47f4b2285593a800d7ac","title":"西瓜书+南瓜书 第3章 线性判别分析","content":"线性判别分析（LDA）概述\r\n定义\r\n线性判别分析（Linear Discriminant Analysis,\r\nLDA）是一种用于分类和降维的统计技术。它通过在特征空间中寻找一个最佳投影方向，使得不同类别的样本尽可能分开。\r\n提出者\r\nLDA最早在二分类问题上由Fisher于1936年提出，因此有时被称为Fisher线性判别。不过，Fisher线性判别没有做出类似LDA所作的假设，比如正态分布的各类或者相等的类协方差。\r\n基本原理\r\n目的\r\nLDA的目的是在特征空间中找到一个直线或超平面，使得不同类别的样本在投影后尽可能分开。在二维空间中，LDA试图找到一条直线，使得正例和反例在这条直线上的投影尽可能分开。\r\n\r\n\r\nLDA图示\r\n\r\n数学模型\r\n类内散度矩阵（Within-class\r\nScatter Matrix）\r\n衡量同类样本的离散程度：\r\n\r\n类间散度矩阵（Between-class\r\nScatter Matrix）\r\n衡量不同类别样本之间的分离程度：\r\n\r\n目标函数与优化\r\n广义瑞利商\r\nLDA的目标是最大化类间散度与类内散度的比值，即：\r\n\r\n求解方法\r\n通过拉格朗日乘子法求解优化问题。具体方法是：\r\n\r\n构建拉格朗日函数。\r\n求导数并设为零，得到特征值问题。\r\n通过奇异值分解（SVD）求解数值稳定的特征向量。\r\n\r\n实现细节\r\n奇异值分解（SVD）\r\n为了数值稳定性，通常对  进行奇异值分解，然后求解\r\n\r\n的特征向量。\r\n投影矩阵\r\n多分类LDA可以通过求解广义特征值问题得到投影矩阵  ，将样本投影到低维空间。\r\n多分类LDA\r\n全局散度矩阵\r\n在多分类情况下，定义全局散度矩阵：\r\n\r\n其中， \r\n是所有样本的均值向量。\r\n多分类优化目标\r\n可以采用不同的散度矩阵组合来实现多分类LDA，一般实现采用优化目标：\r\n\r\n贝叶斯决策理论\r\n理论基础\r\nLDA可以从贝叶斯决策理论角度解释，当数据满足高斯分布且协方差相等时，LDA能够达到最优分类效果。\r\n应用场景\r\n分类\r\nLDA适用于二分类和多分类问题，尤其是在数据集的协方差矩阵相同的情况下。\r\n降维\r\nLDA可以作为一种监督降维技术，减少数据维度同时保留类别可分性。\r\n总结\r\nLDA是一种强大的线性分类和降维工具，它通过最大化类间和类内散度的比值来找到最佳的投影方向。这种方法不仅适用于二分类问题，也可以扩展到多分类任务。LDA的实现通常涉及复杂的数学运算，如拉格朗日乘子法和奇异值分解，但这些技术确保了LDA在实际应用中的有效性和稳定性。\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n线性判别分析\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第3章-线性判别分析","date":"2024-08-19T12:10:58.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习,LDA","author_index":"xiryg"},{"id":"5f8452f3ec8a011a60519ef3438a2291","title":"西瓜书+南瓜书 第3章 线性回归","content":"线性回归\r\n线性回归（linear\r\nregression）是一种用于预测回归问题的算法.\r\n一、线性回归模型\r\n线性回归是对“目标变量随着某个特征变量的增大而增大（或者减小）”这种关联性建模的方法。其假设目标变量\r\n 是自变量 \r\n的线性函数。可以分为一元线性回归和多元线性回归。\r\n\r\n1. 一元线性回归\r\n一元线性回归是最简单的线性回归模型，它只有一个自变量 。其数学表达式为：\r\n\r\n其中，\r\n是斜率（或叫权重）， 是截距。\r\n2. 多元线性回归\r\n多元线性回归扩展了一元线性回归，它包含多个自变量。其数学表达式为：\r\n\r\n用向量表示为：\r\n\r\n其中，，。\r\n二、损失函数和最优化算法\r\n\r\n在线性回归中，常用的损失函数是均方误差（Mean Squared Error,\r\nMSE）。均方误差衡量了预测值与实际值之间的差异，其公式为：\r\n\r\n最优化算法的目标是找到使损失函数最小的参数  和 。基于均方误差最小化来进行模拟求解的方法称为\r\n“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。\r\n三、最小二乘法\r\n最小二乘法通过最小化预测值与实际值之间的平方误差来求解参数。下面详细介绍一元线性回归和多元线性回归中最小二乘法的计算。\r\n1. 一元线性回归的最小二乘法\r\n对于一元线性回归，损失函数为：\r\n\r\n对  和  求导得到：\r\n\r\n上式推导过程如下：\r\n\r\n\r\n令上式为零可得到  和  最优解的闭式解：\r\n\r\n\r\n2. 多元线性回归的最小二乘法\r\n我们把  和  吸收入向量形式 ：\r\n\r\n\r\n对于多元线性回归，损失函数为：\r\n\r\n对 \r\n求导得到：\r\n\r\n令上式为零可得：\r\n\r\n四、代码实现\r\nimport numpy as np\n\n# 构建数据集\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])  # 输入变量\ny = np.array([1, 2, 2, 3])  # 目标变量\n\n# 添加偏置项\nX_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n# 最小二乘法求解\nbeta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\n# 预测\nX_new = np.array([[0, 2], [2, 5]])\nX_new_b = np.c_[np.ones((X_new.shape[0], 1)), X_new]\ny_predict = X_new_b.dot(beta_best)\n\nprint(\"预测结果:\", y_predict)\r\n五、总结\r\n线性回归通过最小二乘法来最小化预测值和实际值之间的误差，从而得到模型参数。一元线性回归适用于单一特征，多元线性回归适用于多特征。\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第3章-线性回归","date":"2024-08-19T12:10:42.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习,线性回归","author_index":"xiryg"},{"id":"ae073498b27c317f94bf5e1039f31b6c","title":"西瓜书+南瓜书 第3章 对数几率回归","content":"1. 基本概念\r\n对数几率回归（Logistic\r\nRegression），尽管名字中包含“回归”，但它主要用于分类任务。它通过使用线性回归模型的输出逼近真实标记的对数几率。\r\n2. 对数几率函数\r\n对数几率函数（Logistic\r\nFunction），也称为Sigmoid函数，是一种将线性回归模型的输出转换为概率的函数，其形式为：\r\n 其中， 是特征向量， 是权重向量， 是偏置项，。\r\n3. 几率（Odds）\r\n几率是正例与反例发生概率的比值，用 \r\n表示。\r\n4. 对数几率（Log Odds）\r\n对数几率是几率的自然对数，即 。\r\n5. 模型优化\r\n使用极大似然法来估计模型参数  和 ，即最大化对数似然函数： \r\n极大似然函数可以转换为最小化损失函数（二元交叉熵损失函数），通常使用数值优化算法求解，如梯度下降法或牛顿法。\r\n- 梯度下降法 &gt;\r\n梯度下降法通过迭代更新权重和偏置项来最小化损失函数。\r\n\r\n牛顿法 &gt;\r\n牛顿法是一种更高效的优化算法，它利用函数的二阶导数来加速收敛。其迭代更新公式为：\r\n&gt; \r\n\r\n6. 优点\r\n\r\n对数几率回归不依赖于数据分布的假设，避免了分布假设不准确带来的问题。\r\n它提供了概率预测，对于需要概率信息辅助决策的任务非常有用。\r\nSigmoid函数是可导的凸函数，具有良好的数学性质，使得优化算法易于实现。\r\n\r\n补充\r\n考虑二分类任务，其输出标记 而线性回归模型产生的预测值\r\n\r\n是实值，于是，我们需将实值  转换为\r\n0/1 值。最理想的是“单位阶跃函数”(unit-step function)\r\n \r\n对数几率回归通过以下公式将线性回归的输出转换为对数几率： \r\n由此，我们可以得到概率预测公式：  \r\n在牛顿法中，第 \r\n轮迭代解的更新公式为： \r\n关于\r\n的一阶导数和二阶导数分别为：  \r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n谢文睿 秦州 贾彬彬 . 机器学习公式详解 第 2 版[M].\r\n人民邮电出版社，2023\r\n视频资料：吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导\r\n\r\n\r\n","slug":"西瓜书-南瓜书-第3章-对数几率回归","date":"2024-08-19T12:10:28.000Z","categories_index":"机器学习,学习笔记","tags_index":"逻辑回归,机器学习","author_index":"xiryg"},{"id":"7b63f0d526d642f698af7d118a8b8858","title":"概览西瓜书+南瓜书 1、2章","content":"第 1 章 绪论\r\n1.1 什么是机器学习\r\n\r\n机器学习就是把无序的数据转换成有用的信息。\r\n研究关于\r\n\"学习算法\"(一类能从数据中学习出其背后潜在规律的算法)\r\n的学科。\r\nTom M. Mitchell 的定义： &gt;\r\n一个计算机程序被认为能从经验E中学习，针对某些任务T和性能度量P，如果它在T类任务上的性能P随着经验E的增加而改进。\r\n&gt; -\r\n任务（T）：垃圾邮件分类，即判定一封邮件是否为垃圾邮件。\r\n&gt; -\r\n性能度量（P）：分类的准确率，即分类正确的邮件数量占总邮件数量的比例。\r\n&gt; -\r\n经验（E）：一组带有标签的邮件数据集，其中每封邮件已经被标记为“垃圾邮件”或“正常邮件”。\r\n&gt; &gt; 如果此垃圾分类程序通过越来越多的（经验E）\r\n在（任务T）上面的（性能度量P）不断提高，能么则认为此程序从经验中学习了。\r\n\r\n1.2 机器学习的基本术语\r\n\r\n西瓜数据集\r\n\r\n\r\n\r\n\r\n编号\r\n色泽\r\n根蒂\r\n敲声\r\n好瓜\r\n\r\n\r\n\r\n\r\n1\r\n青绿\r\n蜷缩\r\n浊响\r\n是\r\n\r\n\r\n2\r\n乌黑\r\n蜷缩\r\n浊响\r\n是\r\n\r\n\r\n3\r\n青绿\r\n硬挺\r\n清脆\r\n否\r\n\r\n\r\n4\r\n乌黑\r\n稍蜷\r\n沉闷\r\n否\r\n\r\n\r\n\r\n\r\n样本/示例：上面一条数据集中的一条数据。\r\n属性/特征：「色泽」「根蒂」「敲声」等。\r\n样本空间/属性空间/输入空间：样本的特征向量所在的空间。\r\n特征向量：空间中每个点对应的一个坐标向量。\r\n标记：关于示例结果的信息，如（（色泽=青绿，根蒂=蜷缩，敲声=浊响），好瓜），其中「好瓜」称为标记。\r\n分类：若要预测的是离散值，如「好瓜」，「坏瓜」，此类学习任务称为分类。\r\n回归：若要预测的值是连续的，如 西瓜成熟度\r\n0.95、0.37，此类学习任务成为回归。\r\n假设：学得模型对应了关于数据的某种潜在规律。\r\n真相：样本背后存在的潜在规律自身。\r\n学习过程：是为了找出或逼近真相。\r\n泛化能力：学得模型适用于新样本的能力。一般来说，训练样本越多，越有可能通过学习来获得具有强泛化能力的模型。\r\n监督学习\r\n：训练数据有标记信息，如「分类」和「回归」。\r\n非监督学习 ：训练数据不带有标记信息，如\r\n「聚类」。\r\n\r\n1.3 假设空间\r\n\r\n假设空间：包含所有可能的条件概率分布或决策函数。\r\n\r\n1.4 机器学习三要素\r\n\r\n模型：根据具体问题，确定假设空间。\r\n策略：根据评价标准，确定选取最优模型的策略（通常产出一个“损失函数”）。\r\n算法：求解损失函数，确定最优模型。\r\n\r\n\r\n第2章 模型评估与选择\r\n2.1 经验误差与过拟合\r\n\r\n误差：学习器对样本的实际预测结果与样本的真实值之间的差异。\r\n训练误差：在训练集上的误差。\r\n测试误差：在测试集上的误差。\r\n泛化误差：学习器在所有新样本上的误差。\r\n过拟合：学习器把训练样本不太一般的特性都学到了。\r\n欠拟合：学习器连训练样本的一般性质都未学好。\r\n\r\n\r\n2.2 评估方法\r\n\r\n评估方法的目的是选择泛化误差最小的学习器。\r\n\r\n2.2.1 留出法\r\n\r\n将数据集分为训练集和测试集，通常2/3-4/5的样本用作训练。\r\n保持数据分布一致性，采用分层抽样。\r\n重复随机划分，取平均值以提高稳定性。\r\n\r\n2.2.2 交叉验证法\r\n\r\n将数据集分为k个子集，进行k次训练和测试。\r\n每次用k-1个子集训练，剩下的子集测试。\r\n常用k值是10，称为10折交叉验证。\r\n\r\n ### 2.2.3 自助法\r\n自助法是一种评估模型泛化能力的方法，适用于数据集较小的情况。\r\n\r\n数据集构建：给定一个包含  个样本的数据集  。\r\n随机抽样：从 \r\n中随机挑选一个样本，将其拷贝放入新数据集  ，然后将样本放回 \r\n，以便在下一次抽样中仍有机会被选中。\r\n重复过程：重复上述抽样过程  次，得到新数据集  ，它也包含  个样本。\r\n概率计算：在 \r\n次抽样中，任何一个样本始终不被抽到的概率的极限是 \r\n这意味着大约有36.8%的样本没有出现在( D' )中。\r\n训练与测试：使用  作为训练集，而  中未出现在  中的样本组成测试集。\r\n\r\n2.2.4 调参与最终模型\r\n\r\n学习算法有多个参数需要调节。\r\n选定参数范围和步长，进行训练和评估。\r\n调参完成后，使用整个数据集重新训练模型。\r\n\r\n2.3 性能度量\r\n\r\n性能度量用于评估模型的好坏。\r\n\r\n2.3.1 错误率与精度\r\n\r\n错误率：分类错误的样本数占总样本数的比例。\r\n精度：分类正确的样本数占预测为正类的样本数的比例。\r\n\r\n2.3.2 查准率、查全率与F1\r\n在分类任务中，特别是在二分类问题中，查准率（Precision）和查全率（Recall）是两个重要的性能度量指标。它们的定义如下：\r\n\r\n\r\n查准率（Precision）：预测为正例且实际为正例的样本数占所有预测为正例样本数的比例。它反映了模型预测为正例中的准确率。\r\n 其中，TP是真正例的数量，FP是假正例的数量。\r\n查全率（Recall）：预测为正例且实际为正例的样本数占所有实际为正例样本数的比例。反映了模型能够找出所有正例的能力。\r\n\r\n其中，FN是假反例的数量。\r\nF1：基于查准率和查全率的调和平均数，是两者的平衡指标，特别适用于查准率和查全率之间存在权衡的情况。\r\n\r\n\r\n\r\n参考文献：\r\n\r\n\r\n周志华. 机器学习[M]. 北京：清华大学出版社，2016.\r\n李航. 统计学习方法[M]. 北京：清华大学出版社，2012.\r\n视频资料：第 1 章\r\n绪论\r\n\r\n\r\n","slug":"概览西瓜书-南瓜书-1、2章","date":"2024-08-19T12:08:32.000Z","categories_index":"机器学习,学习笔记","tags_index":"机器学习","author_index":"xiryg"}]